---
title: "R Steinschlag_Datavisualization"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---
## Introduction

In this R notebook we are going to explore the data of the "Steinschlag-Challeng, HS19C4". The Idea behind the project is to calculate the probablilty of a death through a rockfall at a slope above a road in Graubünden, Switzerland. The Challenge itself can be found under the following link: [Steinschlag Challenge](https://ds-spaces.technik.fhnw.ch/steinschlag/)

This notebook is used to visualize the data through numerous plots. The calculation of the probability will be calculated in a simulation and the final findings of this exploration will be published later on a website.
```{r message = FALSE, echo = TRUE}
library(tidyverse)
library(grid)
library(gridExtra)
library(chron)
library(psych)
library(knitr)
library(MASS)
library(fitdistrplus)
library(propagate)
library(evd)
library(reticulate)
library(sys)
library(janitor)
library(lubridate)
# Go to Working Directory
getwd()
# Import Datasets:
out_1 <- read.csv("out_1.csv", sep = ";")
out_2 <- read.csv("out_2.csv", sep = ";")
traffic_density <- read.csv("trafficdensity_per_hour.csv", sep = ";")
```
## 1. Data transformation

### 1.1 `rename` CSV-Colums
```{r message=FALSE, echo = TRUE}
out_1 <- rename(out_1, mass = Masse..kg., speed = Geschwindigkeit..m.s., time = Uhrzeit, date = Datum, energy = Kin_Energy..kJ.)
out_2 <- rename(out_2, mass = Masse..kg., speed = Geschwindigkeit..m.s., time = Uhrzeit, date = Datum, energy = Kin_Energy..kJ.)
```
### 1.2 Transform 'traffic_density' set

The traffic_density data is scaled to 217.0795 percent and needs to be scaled down to 100 percent. Because of that we divide the percentile column by 2.170795
```{r echo = TRUE}
sum(traffic_density$percentile)
traffic_density <- mutate(traffic_density, percentile = percentile/2.170795 )
sum(traffic_density$percentile)
```

### 1.3 Transform time column into datetime-type

The code below could be used to transform the time column of our dataset into a datetime type.
```{r echo = TRUE}
# out_1 <- transform(out_1, time = as.times(time))
# out_2 <- transform(out_1, time = as.times(time))
```
### 1.4 Combine Datasets
```{r echo = TRUE}
#Now that we have two datasets with the same column names we can combine them into one:
data_bind <- rbind(out_1, out_2)
```
### 1.5 Delete rows with mistakes

In the dataset we see that one event has a mass of zero. Either the rock has a mass below 1 kg or the value is a mistake in the dataset. Because we can't now what is true we are going to work with a subset of our original data that doesn't contain the  row with a mass value of zero:
```{r echo = TRUE}
rockfall <- subset(data_bind, mass != 0)
```

### 1.6 Statistic values

Here one can see some statistical values such as mean, standard deviation (sd), median, min and max value.
```{r echo = TRUE}
mass <- dplyr::select(rockfall, mass)

stat_data <- dplyr::select(describe(dplyr::select(rockfall, mass, speed, energy)), mean, sd, median, min, max)
stat_data
```

## 2. Datavisualization
### 2.1 Histogramms

#### 2.1.1 Mass

The code below plots a histogramm of the "mass"-column in our dataset. Here we see that most of the rocks have a mass below 500kg. This is intersting because we would need more than four rocks of this size to break through the security-net. If we look at other variables we see that we only have two days with this many rockfalls.(The next histogramms are plottet with the same code, just with other variables)
```{r echo = TRUE, warning = FALSE}
ggplot(data = rockfall)+
  # Plot histogram of the mass distribution in the rochfall dataset:
  geom_histogram(mapping = aes(x =mass), fill = "steelblue")+
  labs(title = "Distribution of mass", x='mass [kg]')
```
#### 2.1.2 Energy

If we plot the energy (in kilojoule) we can see that a big part of our rocks have a very low energy and we can even see that over 15 have a value of 0 on the plot. This is because we have multiple rocks that don't reach an energy of >10kJ.
```{r echo = TRUE}
ggplot(data = rockfall)+
  geom_histogram(mapping = aes(x = energy),fill = "steelblue")+
  labs(title = "Distribution of energy", x='energy [kJ]')
```
#### 2.1.3 Speed

Here we see that we have a maximum speed of around 46 meters per second. The speed is also split into two groups which indicates shows us the that the two places where rockfalls occur are on a diffrent hight. Later in this notebook I'm going to plot the relationship of the mass and speed.
```{r echo = TRUE}
ggplot(data = rockfall)+
  geom_histogram(mapping = aes(x = speed),fill = "steelblue")+
  labs(title = "Distribution of speed", x='speed [m/s]')
```
#### 2.1.4 Rockfalls per Hour

As soon as we group the events to the hour they occur we see that we have an increase of rockfalls towards noon and a slower decrease after noon. The most rocks fall at 12 O'clock. At this time the traffic will be high as well, wich means that we should include the traffic density in to our probability calculation.
```{r echo = TRUE}
# count <- select(rockfall, time = n())
ggplot(data = rockfall)+
  geom_histogram(mapping = aes(x = time),fill = "steelblue", stat="count")+
  coord_flip()+
  labs(title = "Total number of Events per Hour")
```


### 2.2 Graphs

#### 2.2.1 False readouts

Something interesting we found was that the binding of the two datasets can lead to some errors in the datavisualization. The first two graphs below show us how the mass corresponds to the energy of a rockfall. Which seems to be quite linear. This makes sense so far. As soon as we plot the same two variables out of the combined dataset(`rockfall`) we get a spike around 300 to 500 kg. This doesn't seem to make sense. But this tells us that the rocks of the two dataset have to fall from diffrent hights. Because only a greater speed in one dataset would explain the rise of energy in the mass. This also tells us that we can't use the data as a combined dataset and should work with the two seperate datasets. Especially to fit a distribution to the data.
```{r echo = TRUE, fig.width = 10}
ggplot(data = out_1)+
  geom_smooth(mapping = aes(x = mass, y = energy), color = "blue") +
  ggtitle("out_1 Dataset")+
  labs(x='mass [kg]')-> p1

ggplot(data = out_2)+geom_smooth(mapping = aes(x = mass, y = energy), color = "blue") +           ggtitle("out_2 Dataset")+
  labs(x='mass [kg]')-> p2

ggplot(data = rockfall)+
  geom_smooth(mapping = aes(x = mass, y = energy), color = "red") + 
  ggtitle("rockfall Dataset")+
  labs(x='mass [kg]')-> p3
grid.arrange(p1, p2, p3, ncol = 2, nrow = 2)
```
#### 2.2.2 Traffic density per hour

The following graph shows the trafiic density on roads in switzerland per hour. This data will be used to calculate the probability of a hit combined with the traffic density. This data has been pulled from the swiss institute for statistics and contains the information of the average of cars passing through per hour. This dataset is from 2015 but is the latest data with this information.
```{r echo = TRUE}
ggplot(data = traffic_density)+
  geom_step(mapping = aes(x = hour, y = percentile), color = "blue")+
  ggtitle("Distribution of Traffic througout the day")
```

### 2.3 Scatterplot

This visualization shows us how the mass and the speed of our rocks corellate. Here we can also see a clear diffrence between the two dataset. (The Energy is also visible as the size and color of the dots. A darker color means less energy.)

```{r echo = TRUE, fig.width = 10}

ggplot(data = rockfall, aes(x = speed, y = mass, size = energy, color = energy ))+
  geom_point()+
  ggtitle("Correlation of mass and speed")+
  labs(x = 'speed [m/s]', y = 'mass [kg]')

```



## 3. Calculation of the probability of a car getting hit by a rock under the assumption that a rock could break trough the securitynet.

Given:

- Rock breaks through net
- Mass of stone is over two tonnes
- Trafficdensity is 1200 cars per day, without any trafficjam
- Speed of the cars: 16.66 m/s
- Average size of car:
  - Length: 4.4m
  - Width: 1.8m

Assumptions:

- A car getting hit by stone automaticaly causes a deady accident
- Higth of car wont be part of the calculation
- Speed of the rock wont be part of the calculation
- Width of rock is on average about 1 meter

### 3.1 Probability of Rockfall per Hour

To calculate the expected value at which hour a rockfall takes place we are going to take a look at the Histogram "Rockfalls per Hour" (2.1.4) which shows us the distribution throughout the day. To count the falls per hour we are going to use the function `count` and divide these numbers by 0.99 to get the percentage which also equals the expected value of an event occuring at that hour.
```{r echo = TRUE}
exp_rock <- count(rockfall, time)

(exp_rock <- mutate(exp_rock,
                 expected_value = n/0.99/100))
```

#### 3.2 Total Time a Car is in Danger

The next step is to calculate the time a car is in the dangerous zone. We now that the cars drive with 60 km/h and have an average length of 4.4 meters. Because the car can be hit in the front or in the back we need to take double the length plus the width of the stone as our danger zone. Which comes out to be 9.8 meters. With this we calculate the following:
```{python echo = TRUE, message=TRUE}
#Time to drive a distance of 9.8 meters (2* the length of cars + length of rockd)

speed = 60/3.6 #conversion form km/h to m/s

#time will be calculatet with the formula "distance/speed"
T_single_car = 9.8/speed
print("Time to drive 9.8m :", T_single_car, "seconds")
```
Now that we have the time a car is in the "danger"-zone we can calculate the total time (seconds in hour) in which cars are in danger of getting hit by a rock breaking though the net.
For this we are going to create a new list called "car_per_hour" which will contain the total count of cars driving through, the total time of cars being in danger and the percentage of a car driving through this zone at this hour.

After that we can calculate the expected value of a car getting hit by simply multiplying the probability of a rock falling at this hour and the probability of a car being in the dangerous zone at this hour.
We calculate an expected value of 0.009241006 or 0.924% for this event to happen.

```{r echo = TRUE}
# calculate cars passing though per hour
car_per_hour <- mutate(traffic_density,
                       cars_passing = percentile * 12,
                       time_in_danger = cars_passing * 0.588,
                       exp_car = time_in_danger/3600)


(expected_value_car <- mutate(exp_rock, time, n, expected_value,
                          car_passing_hour = car_per_hour$cars_passing,
                          t_car_in_danger = car_per_hour$time_in_danger,
                          exp_car_per_hour = car_per_hour$exp_car,
                          t_exp_per_hour = expected_value * car_per_hour$exp_car))

total_in_danger <- dplyr::select(expected_value_car, exp_car_per_hour)
```

```{r}
expected_value_car <- mutate(expected_value_car,
hour = traffic_density$hour)

ggplot(data = expected_value_car)+
  geom_step(mapping = aes(x = hour, y = expected_value, color = '... of rockfall'), linetype = "dashed")+
  geom_step(mapping = aes(x = hour, y = exp_car_per_hour, color = "... of cars passing"), linetype = "twodash")+
  geom_step(mapping = aes(x = hour, y = t_exp_per_hour, color = "... of a car getting hit"))+
  labs(title='Expected Values...')
```
**Read description**: The blue line in the graph shows the expected value of a breakthrough per hour. The maximum expectation value here is 0.1 at 12 o'clock noon. The green line indicates the expectation value of a car being in the danger zone at the same time of a rock fall. If these values are calculated, the expected value of a hit (red line) results.

The expected value of a car getting hit by rockfall at the assumption that a rockfall, capable of breaking through the net, occurs is:
```{r echo = TRUE}
# the probability of a car getting hit by a rock incase it breaks through the security net.
e_car_hit <- sum(expected_value_car$t_exp_per_hour)
e_car_hit
```

## 4. Net-Energy

### 4.1 Net-Energy Distribution

We aim to find an adequate density and distribution function based on the energy of both given data-sets `out_1`" and `out_2`" to be able to calculate the $P(X \geq x | stone falls into the net)$ where $X$ is a continuous variable of the calculated energy $energy[kJ] = \frac{0.5 * m * v^2}{1000}$. Since there is just one net and the energy distribution of the net doesn't care from wich source the rock falls, both data-sets are considered to evaluate an appropriate density funciton.

To get an imagination of how the data might be distributed, we plot the density of the observed energy with a histogramm.

The comparison of different density functions with the approximated function of the observed energy shows how well each function fit the actual values of the rockfall.
```{r echo = TRUE, warning = FALSE, fig.width = 12}
e_fit <- fitdistr(rockfall$energy, "exponential")
g_fit <- fitdistr(rockfall$energy, "gamma")
lnrm_fit <- fitdistr(rockfall$energy, "lognormal")
w_fit <- fitdistr(rockfall$energy, "weibull")

ggplot(rockfall, aes(energy)) +
  geom_histogram(aes(y = ..density..), binwidth = 5) +
  geom_line(stat = "density", aes(color = "empirical density", linetype = "empirical density")) +
  ggtitle("Energy Distribution of out_1 & out_2") +
  stat_function(fun = dexp, args = list(e_fit$estimate[1]),
                aes(color = "exponential", linetype = "exponential")) +
  stat_function(fun = dgamma, args = list(g_fit$estimate[1], g_fit$estimate[2]),
                aes(color = "gamma", linetype = "gamma")) +
  stat_function(fun = dlnorm, args = list(lnrm_fit$estimate[1], lnrm_fit$estimate[2]),
                aes(color = "log normal", linetype = "log normal")) +
  stat_function(fun = dweibull, args = list(w_fit$estimate[1], w_fit$estimate[2]),
                aes(color = "Weibull", linetype = "Weibull")) +
  scale_color_manual(name = "",
                     values = c("empirical density" = "black",
                                "exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("empirical density",
                                "exponential",
                                "gamma",
                                "log normal",
                                "Weibull")) +
  scale_linetype_manual(name = "",
                        values = c("empirical density" = "solid",
                                   "exponential" = "dashed",
                                   "gamma" = "dashed",
                                   "log normal" = "dashed",
                                   "Weibull" = "dashed"),
                        breaks = c("empirical density",
                                   "exponential",
                                   "gamma",
                                   "log normal",
                                   "Weibull")) +
  labs(x = "energy [kJ]")
```
### 4.2 Net-Energy Density Function Evaluation

Now we have to figure out which distribution fits our energy data best.
QQ_plots have the purpose to compare both, theoretical quantiles of the given distribution on the y-axis vs. the actual quantiles of the sample on the x-axis. The more the quantiles match, the closer the data points approach a straight line. The red line placed over the plot describes the line which the data points should approximate if they are distributed perfectly.

We figured out that the log normal distribution seems to have a very good fit on the energy distribution on the net. The comparison shows, that the exponential, gamma, and weibull distribution doesn't fit very well in regards to the extreme values.
It is important to mention that each data point does not have a connection as one can see in the QQ-plot. But it helps to get a better overview of the data points.
```{r echo = TRUE, fig.width = 10}
ggplot(rockfall, aes(sample = energy)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qgamma,
          dparams = list(g_fit$estimate[1], g_fit$estimate[2]),
          geom = "line",
          aes(color = "gamma")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit$estimate[1], lnrm_fit$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit$estimate[1], w_fit$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "gamma",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Net-Energy", x = "theoretical quantiles [kJ]", y = "sample quantiles [kJ]")
```

### 4.3 Net-Energy Cumulative Distribution Function

Here one can see that the cumulative distribution of the log normal function matches the empiric values best.
```{r echo = TRUE, fig.width = 10}
ggplot(rockfall, aes(energy)) +
  stat_ecdf(aes(color = "empirical cumulative distribution",
                linetype = "empirical cumulative distribution")) +
  stat_function(fun = pexp, args = list(e_fit$estimate),
                aes(color = "exponential", linetype = "exponential")) +
  stat_function(fun = pgamma, args = list(g_fit$estimate[1], g_fit$estimate[2]),
                aes(color = "gamma", linetype = "gamma")) +
  stat_function(fun = plnorm, args = list(lnrm_fit$estimate[1], lnrm_fit$estimate[2]),
                aes(color = "log normal", linetype = "log normal")) +
  stat_function(fun = pweibull, args = list(w_fit$estimate[1], w_fit$estimate[2]),
                aes(color = "Weibull", linetype = "Weibull")) +
  scale_color_manual(name = "",
                     values = c("empirical cumulative distribution" = "black",
                                "exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("empirical cumulative distribution",
                                "exponential",
                                "gamma",
                                "log normal",
                                "Weibull")) +
  scale_linetype_manual(name = "",
                        values = c("empirical cumulative distribution" = "solid",
                                   "exponential" = "dashed",
                                   "gamma" = "dashed",
                                   "log normal" = "dashed",
                                   "Weibull" = "dashed"),
                        breaks = c("empirical cumulative distribution",
                                   "exponential",
                                   "gamma",
                                   "log normal",
                                   "Weibull")) +
  labs(title = "CDF Net-Energy", x = "energy [kJ]", y = "cumulative probability")
```

### 4.4 Net-Energy Log-Norm Distribution Validation

To be sure, we test the sample if it is log normal distributed. We assume $(H_0)$ that our data is log normal distributed with significant level $95\%$. $H_1$ is that the energy of the data set is not log normal distributed. Therefore, we compare 20 smples of random generated log normal distributed quantiles vs. the theoretical quantiles. If there is a similar QQ-plot compared to the plot with the actual energy data, then we can assume that the energy of the rockfall is log-normal distributed.

The comparison of the artificial created data sample and the data set of the energy let us assume, not to deny $H_0$. For this reason, we can transform the energy values with the $ln(x)$ to get a normal distribution in order to compute the probability with the normal distribution function.
```{r echo = TRUE, fig.width = 6}
rows <- 20
cols <- length(rockfall$energy)
random_dlnorm_matrix <- matrix(nrow = rows, ncol = cols)
for (i in 1:rows){
  random_dlnorm_matrix[i,] <- c(rlnorm(cols, meanlog = lnrm_fit$estimate[1], sdlog = lnrm_fit$estimate[2]))
}
for (j in 1:rows){
  p <- ggplot2::qplot(sample = random_dlnorm_matrix[j,], geom = "blank") +
    stat_qq(distribution = qlnorm, dparams = list(lnrm_fit$estimate[1], lnrm_fit$estimate[2]), color = "steelblue", size = 3) +
    stat_qq_line(distribution = qlnorm, dparams = list(lnrm_fit$estimate[1], lnrm_fit$estimate[2]), color = "red") +
    labs(title = paste0("Random Log-Normal distributed Energy (img ", j, ")"), x = "theoretical quantiles [kJ]", y = "sample quantiles [kJ]")
  print(p)
}
```

### 4.5 Probability of a Rockfall in a certain Energy Interval

Assuming to have a log normal distribution, we are now able to calculate the probability $P(500 \leq x \leq 1000 | A)$ and $P(x \geq 1000 | A)$ of an interval where $A$ is the condition that a rock falls into the net. $P(0 \leq x < 500 | A)$ is not interesting since a rock with energy $x < 500$ is never strong enough to break through the net. First we compute the natural logarithm of each value on the rockfall energy set $ln(x_i)$ To be able to calculate the probability, the log normal distribution needs $meanlog$ and the $sdlog$ of the distribution, which can be calculated from the logarithmized values of the energy data set.

Now we can take the cumulative distribution function of normal distribution ${\displaystyle F(x)=\int _{-\infty }^{x}{\frac {1}{\sqrt {2\pi }*\sigma}}e^{-\frac{1}{2}(\frac{t-\mu}{\sigma})}\,dt}$ if we transform each energy values with the natural logarithm. To calculate e.g. $P(x \geq 1000 | A)$ we are now able to calculate the area between $0$ and $ln(1000)$ with the normal distribution if we take $\mu = meanlog$, $\sigma = sdlog$ and then subtract the value from $1$ to get the area after $1000 kJ$.
```{r echo = TRUE}
#fitDistr(rockfall$energy, nbin = 99, plot = "qq") ~5min
prob_lnorm <- stats::plnorm(c(500, 1000), meanlog = lnrm_fit$estimate[1], sdlog = lnrm_fit$estimate[2], lower.tail = TRUE)
prob_lnorm_500_to_1000 <- (prob_lnorm[2] - prob_lnorm[1])
prob_lnorm_1000 <- (1 - prob_lnorm[2])
print(paste('P(500 < x < 1000 | A) =', prob_lnorm_500_to_1000))
print(paste('P(1000 < x | A) =', prob_lnorm_1000))
```
```{r echo = TRUE}
day_mass <- 0
```

## 5. Mass on the Net Distribution

### 5.1 Time Series Mass in Net Creation

Furthermore, we want to know the probability that the net has rock-weights equals or more than 2000 kg $P(x \geq 2000 | A)$. Therefore, a new dataframe must be created with the cumulative weight for each day and hour in the net. We assume, that the net was cleared every day at 08:00.
```{r echo = TRUE}
rockfall_ordered <- rockfall
rockfall_ordered$date <- base::as.Date(rockfall$date, format = "%d.%m.%Y")
rockfall_ordered <- rockfall_ordered[order(rockfall_ordered$date),]
num_of_days <- difftime(rockfall_ordered$date[base::nrow(rockfall_ordered)], rockfall_ordered$date[1], units = "days")
day_first_observation <- rockfall_ordered$date[1]
day_times <- c("00:00:00", "01:00:00", "02:00:00", "03:00:00", "04:00:00", "05:00:00", "06:00:00", "07:00:00", "08:00:00", "09:00:00", "10:00:00", "11:00:00", "12:00:00", "13:00:00", "14:00:00", "15:00:00", "16:00:00", "17:00:00", "18:00:00", "19:00:00", "20:00:00", "21:00:00", "22:00:00", "23:00:00")
clearing_time <- "08:00:00"

#initialize first value of vector to ensure correct data type, must be sliced afterwards
day <- c(day_first_observation)
time <- c("00:00:00")
mass <- c(rockfall_ordered$mass[1])


#loop through every day since the first day of observation
for (i in 0:num_of_days){
  today <- day_first_observation + (i)
  selected_frame_day <- dplyr::filter(rockfall_ordered, rockfall_ordered$date == today)
  #loop through every hour of the day and add each observed weight of a day
  for (hour in day_times){
    if (hour == clearing_time){
      day_mass = 0
    }
    selected_frame_hour <- dplyr::filter(selected_frame_day, selected_frame_day$time == hour)
    if (base::nrow(selected_frame_hour) > 0){
      day_mass = day_mass + base::sum(selected_frame_hour[3]) #sum if the vector contains multiple values
    }
    day <- c(day, today)
    time <- c(time, hour)
    mass <- c(mass, day_mass)
  }
}
net_weigt <- base::data.frame(day, time, mass)
net_weigt <- net_weigt[-1,] #delete first row from the frame
```

This is the load of the net of each hour since the beginning of the observation.
```{r echo = TRUE, fig.width = 10}
hours_since_observation <- c(1:nrow(net_weigt))
ggplot2::ggplot(data = net_weigt) +
  geom_area(mapping = aes(x = hours_since_observation, y = mass), fill="steelblue") +
  labs(title = "Time Series Mass on the Net (daily clearing 08:00)", x = "hour since observation", y = "mass [kg]")
```

### 5.2 Mass on the Net Distribution

Now we aim to find a distribution funciton which fits the actual mass data on the net best. In addition, we need the data to be independant. Therefore, to be on the safe side, we just considered the max weight before clearing.

After several tries we did not find out a distribution that fits the weigt on the net well. We decided to try another approach to calculate an appropriate probability that a rock falls through the net.
```{r echo = TRUE}
#gets the last cumulative load on the safety net
index_of_clearing_time <- base::match(c(clearing_time), day_times)
if (index_of_clearing_time == 1){
  index_of_clearing_time = length(day_times)
} else {
  index_of_clearing_time = index_of_clearing_time -1
}
cumulative_weight_per_period <- dplyr::filter(net_weigt, time == day_times[(index_of_clearing_time)])

ggplot2::ggplot(data = cumulative_weight_per_period, mapping = aes(x = mass)) +
  geom_histogram(mapping = aes(y = ..count..), binwidth = 20, fill = "steelblue") +
  labs(title = "Mass on Net Distribution", x = 'mass [kg]')
```

## 6. Mass & Speed Distribution

### 6.1 Mass Distribution out_1

Our next approach is to get the probability with the help of a Monte-Carlo simulation. Therefore, we have to find out the distributions of the weight and speed of a rockfall of each data set `out_1` and `out_2`. In addition, we need the time from an event (rockfall) to the next to set up the simulation.

```{r echo = TRUE}
ggplot(out_1, aes(mass)) +
  geom_histogram(aes(y = ..density..), binwidth = 40, fill = "steelblue") +
  labs(title = "Mass out_1", x = "mass [kg]")
```

### 6.2 Mass Distribution out_2

```{r echo = TRUE}
out_2$mass <- base::replace(out_2$mass, out_2$mass == 0, 1) #replace 0 values of the observation to be able to avoid arithmetic issues
ggplot(out_2, aes(mass)) +
  geom_histogram(aes(y = ..density..), binwidth = 20, fill = "steelblue") +
  labs(title = "Mass out_2", x = "mass [kg]")
```

### 6.3 Mass out_1 Density Function Evaluation

Now we check what distribution fits the mass of `out_1` best. Therefore, we compare exponential, gamma, log-normal and weibull.
We can see that the log-normal distribution fits our data best except of the last data point. Even if the last data point is completely not correct estimated, the log normal distribution esitmated the weight higher then the actual weight was, which was estimated more cautiously.
```{r echo = TRUE, fig.width = 10, warning = FALSE}
e_fit_mass_out_1 <- fitdistr(out_1$mass, "exponential")
g_fit_mass_out_1 <- fitdistr(out_1$mass, "gamma")
lnrm_fit_mass_out_1 <- fitdistr(out_1$mass, "lognormal")
w_fit_mass_out_1 <- fitdistr(out_1$mass, "weibull")

ggplot(out_1, aes(sample = mass)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_out_1$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qgamma,
          dparams = list(g_fit_mass_out_1$estimate[1], g_fit_mass_out_1$estimate[2]),
          geom = "line",
          aes(color = "gamma")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit_mass_out_1$estimate[1], lnrm_fit_mass_out_1$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_out_1$estimate[1], w_fit_mass_out_1$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "gamma",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Mass out_1", x = "theoretical quantiles [kg]", y = "sample quantiles [kg]")
```

### 6.4 Mass out_2 Density Function Evaluation

Now we aim to find an appropriate distribution funciton for the mass of `out_2`. Therefore, we compare exponential, lognormal and weibull.
In regards to the result of the QQ-plot, we decided to consider the exponential distribution function, which is pretty similar distributed compared to the weibull function.
```{r echo = TRUE, fig.width = 10}
e_fit_mass_out_2 <- fitdistr(out_2$mass, "exponential")
lnrm_fit_mass_out_2 <- fitdistr(out_2$mass, "lognormal")
w_fit_mass_out_2 <- fitdistr(out_2$mass, "weibull")

ggplot(out_2, aes(sample = mass)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_out_2$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit_mass_out_2$estimate[1], lnrm_fit_mass_out_2$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_out_2$estimate[1], w_fit_mass_out_2$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "gamma",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Mass out_2", x = "theoretical quantiles [kg]", y = "sample quantiles [kg]")
```

### 6.5 Speed Distribution out_1

Now we are interested in how the speed of `out_1` and `out_2` is distributed.
```{r echo = TRUE}
ggplot(out_1, aes(speed)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "steelblue") +
  labs(title = "Speed out_1", x = "speed [m/s]")
```

### 6.6 Speed Distribution out_2

```{r echo = TRUE}
ggplot(out_2, aes(speed)) +
  geom_histogram(aes(y = ..count..), binwidth = 2, fill = "steelblue") +
  labs(title = "Speed out_2", x = "speed [m/s]")
```

### 6.7 Speed out_1 Density Function Evaluation

We can assume to have a normal distribution in regards to the histogram of `out_1`. For this reason, we just check, if the speed data fits a normal or logistic distribution.
In the QQ-plot we can see that both, normal and logistic distribution fits the speed of `out_1` very well. We continue with the assumtion of having a normal distribution.
```{r echo = TRUE, fig.width = 10, warning = FALSE}
nrm_fit_speed_out_1 <- fitdistr(out_1$speed, "normal")
logis_fit_speed_out_1 <- fitdistr(out_1$speed, "logistic")

ggplot(out_1, aes(sample = speed)) +
  stat_qq(distribution = qnorm,
          dparams = list(nrm_fit_speed_out_1$estimate[1], nrm_fit_speed_out_1$estimate[2]),
          geom = "line",
          aes(color = "normal")) +
  stat_qq(distribution = qlogis,
          dparams = list(logis_fit_speed_out_1$estimate[1], logis_fit_speed_out_1$estimate[2]),
          geom = "line",
          aes(color = "logistic")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("normal" = "red",
                                "logistic" = "blue"),
                     breaks = c("normal",
                                "logistic")) +
  labs(title = "Q-Q Speed out_1", x = "theoretical quantiles [m/s]", y = "sample quantiles [m/s]")
```

### 6.8 Speed out_2 Density Function Evaluation

To estimate the speed of the sample `out_2`, we set up another QQ-plot with possible distributions like normal, logistic and gamma distribution.

Here, it is not very clear which distribution fits best or even if a selected distribution is appropriate. We assume that our speed data of `out_2` could be normal distributed. Therefore, we test, if with the given $\mu$ and $\sigma$ a normal distribution is possible.
```{r echo = TRUE, fig.width = 10}
nrm_fit_speed_out_2 <- fitdistr(out_2$speed, "normal")
logis_fit_speed_out_2 <- fitdistr(out_2$speed, "logistic")
gamma_fit_speed_out_2 <- fitdistr(out_2$speed, "gamma")

ggplot(out_2, aes(sample = speed)) +
  stat_qq(distribution = qnorm,
          dparams = list(nrm_fit_speed_out_2$estimate[1], nrm_fit_speed_out_2$estimate[2]),
          geom = "line",
          aes(color = "normal")) +
  stat_qq(distribution = qlogis,
          dparams = list(logis_fit_speed_out_2$estimate[1], logis_fit_speed_out_2$estimate[2]),
          geom = "line",
          aes(color = "logistic")) +
  stat_qq(distribution = qgamma,
          dparams = list(gamma_fit_speed_out_2$estimate[1], gamma_fit_speed_out_2$estimate[2]),
          geom = "line",
          aes(color = "gamma")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("normal" = "red",
                                "logistic" = "blue",
                                "chi-squared" = "purple",
                                "gamma" = "green"),
                     breaks = c("normal",
                                "logistic",
                                "chi-squared",
                                "gamma")) +
  labs(title = "Q-Q Speed out_2", x = "theoretical quantiles [m/s]", y = "sample quantiles [m/s]")
```

### 6.9 Speed out_2 Gaussian Distribution Validation

## 7. Investigation of the amount of rockfalls
We know the date and the time of any rockfalls. Out of that we can calculate what is the average amount of rockfalls in a day or even in a hour. First we need a proper format for the time calculation. We have now a accurate timestamp vor every rockfall. Than we can subtract the previous Timestamp from every timestampt to get the time between the rockfalls.
````{r}
# Zone 1
out_1

zone_1_Time <- out_1 %>%
  transmute(dateTime = as.POSIXct(paste(date, time), format = "%d.%m.%Y %H:%M:%S"))

zone_1_Time

zone_1_TimeDiff <- zone_1_Time %>%
  mutate(
    diff_secs = as.numeric(dateTime - lag(dateTime))+1,
    diff_mins =  as.numeric(diff_secs)/60,
    diff_hours =  as.numeric(diff_secs)/3600,
    diff_days =  as.numeric(diff_secs)/ 86400)

#Zone 2 
zone_2_Time <- out_2 %>%
  transmute(dateTime = as.POSIXct(paste(date, time), format = "%d.%m.%Y %H:%M:%S"))


zone_2_TimeDiff <- zone_2_Time %>%
  mutate(
    diff_hours = as.numeric (dateTime - lag(dateTime)) +1,
    diff_mins = diff_hours * 60,
    diff_secs = diff_hours * 3600,
     diff_days = diff_secs/ 86400)

zone_1_TimeDiff %>%
  ggplot() +
  geom_histogram(aes(x = diff_hours), binwidth = 10, fill = "steelblue")+
  labs(title='Distribution of time delta between events (zone 1)', x = 'Time delta [hours]')


zone_2_TimeDiff %>%
  ggplot() +
  geom_histogram(aes(x = diff_hours), binwidth = 10, fill = "steelblue")+
  labs(title='Distribution of time delta between events (zone 2)', x = 'Time delta [hours]')


head(zone_1_TimeDiff)
head(zone_2_TimeDiff)

#Der erste Wert muss gelöscht werden. Denn aus der Berechnung der Zeitdiffertenz ergibt sich die Annhame, dass die Messung bei dem ersten Steinschlag begonnen hat, und beim letzten Steinschlag aufgehört hat. 

sum(is.na(zone_1_TimeDiff))
#hat 1 NA Wert
sum(is.na(zone_2_TimeDiff))
#hat 1 NA Wert

zone_1_TimeDiff <- na.omit(zone_1_TimeDiff)
zone_2_TimeDiff <- na.omit(zone_2_TimeDiff)

sum(is.na(zone_1_TimeDiff))
#hat 0 NA Wert
sum(is.na(zone_2_TimeDiff))
#hat 0 NA Wert

#Ergebniss :
# - Eine Merkmalsausprägung musste gelöscht werden, das nun nur noch der Zeitunterschied untersuccht wird.

zone_1_TimeDiff
zone_2_TimeDiff
```

### 7.1 Difference betewen the zones

It shows that there is a big difference between the two datasets.
- The analysis of the time between events has a probability calculation. The cut of the distances is 30.55 hours with stretching from 0 to 133 hours. From the histogram it can be seen that there is a steep rise at the beginning that reaches its maxium at c.a 10 hours and then flattens again. The maximum time intervals between each end is 113 hours, now the question is whether this is also the maximum. I suppose the maximum of the distances is one year, because this is the time because after this time comes the new network.

- On the one hand this merging of "the time in which nothing happened" distorts the result, on the other hand these values are all at 0 anyway. The goal of a time series is to determine a certain distribution by smoothing and co. This is not useful because many values are at 0. Time series are only meaningful if the Y-value never goes to 0.

- Time unit must be checked, perhaps there is a time unit that outputs a symmetricteling. Time units have been extended, dataset now has the time distance between the achievements in seconds, minutes, hours and days.

- Class instead of mass. I could divide the time unit more and more. For the optimal class size there are different approaches. One of them is the root of the number of events. The events are in record 1= 67. In record 2 they are 31. 
  - Class division 1 = 8.18 -> 8
  - Class division 2 = 5.56 -> 6

- Time interval was divided into classes of days. To keep the optimal class width the binwith is set to 0.5. Now there are 10 classes. Which in my opinion is a good class instead of mass.

- Time interval was divided into "regular to each other" classes. Now only the probability has to be calculated. 


```{r}
plot.ts(zone_1_TimeDiff)
plot.ts(zone_2_TimeDiff)

# hier ist jede gemesene Stunde eine Klasse. Ansatz: Klasse statt Masse 
# Doch zuerst wird est mal die Diff_hour untersucht

stat_data_3 <- dplyr::select(describe(dplyr::select(zone_1_TimeDiff, diff_days,diff_hours, diff_mins, diff_secs)), mean, sd, median, min, max)

stat_data_4 <- dplyr::select(describe(dplyr::select(zone_2_TimeDiff, diff_days,diff_hours, diff_mins, diff_secs)), mean, sd, median, min, max)

stat_zone_new2 <- rbind(stat_data_3,stat_data_4)
stat_zone_new2


zone_1_TimeDiff %>%
  ggplot() +
  geom_histogram(aes(x = diff_hours), fill = "steelblue", binwidth = 6)+
  labs(title='Distribution of time delta between events (zone 1)', x = 'Time delta [hours]')

zone_2_TimeDiff %>%
  ggplot() +
  geom_histogram(aes(x = diff_mins), fill = "steelblue")+
  labs(title='Distribution of time delta between events (zone 2)', x = 'Time delta [hours]')
```

### 7.2 The probability of the time difference

- The fitting of the probability distribution does not depend on the time magnitude and therefore also not on the Binwith. 
- It's hard to say which distribution to take.

#### 7.2.1 Result of zone 1
The best fit is the "log normal" distribution, because this distribution is clearly the best at the beginning and even in the area where all distributions begin to become inaccurate it is still best. Only at the last extreme points is the log distribution the worst.
```{r}
# Untersuchung welche Zeiteinheit am besten passt.
# GGplot für Tage

e_fit_mass_zone_2_d <- fitdistr(zone_2_TimeDiff$diff_days, "exponential")
lnrm_fit_mass_zone_2_d <- fitdistr(zone_2_TimeDiff$diff_days, "lognormal")
w_fit_mass_zone_2_d <- fitdistr(zone_2_TimeDiff$diff_days, "weibull")

ggplot(zone_2_TimeDiff, aes(sample = diff_days)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_zone_2_d$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit_mass_zone_2_d$estimate[1], lnrm_fit_mass_zone_2_d$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_zone_2_d$estimate[1], w_fit_mass_zone_2_d$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                               # "gamma" = "blue",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                #"gamma",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Plot in days", x = 'theoretical [days]', y = 'sample [days]')

#GGplot für Stunden

e_fit_mass_zone_2_h <- fitdistr(zone_2_TimeDiff$diff_hours, "exponential")
lnrm_fit_mass_zone_2_h <- fitdistr(zone_2_TimeDiff$diff_hours, "lognormal")
w_fit_mass_zone_2_h <- fitdistr(zone_2_TimeDiff$diff_hours, "weibull")

ggplot(zone_2_TimeDiff, aes(sample = diff_hours)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_zone_2_h$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit_mass_zone_2_h$estimate[1], lnrm_fit_mass_zone_2_h$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_zone_2_h$estimate[1], w_fit_mass_zone_2_h$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Plot in hours", x = 'theoretical [hour]', y = 'sample [hour]')


# QQ-plot für Minuten


e_fit_mass_zone_2_m <- fitdistr(zone_2_TimeDiff$diff_mins, "exponential")
lnrm_fit_mass_zone_2_m <- fitdistr(zone_2_TimeDiff$diff_mins, "lognormal")
w_fit_mass_zone_2_m <- fitdistr(zone_2_TimeDiff$diff_mins, "weibull")

ggplot(zone_2_TimeDiff, aes(sample = diff_mins)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_zone_2_m$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit_mass_zone_2_m$estimate[1], lnrm_fit_mass_zone_2_m$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_zone_2_m$estimate[1], w_fit_mass_zone_2_m$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Plot in minutes", x = 'theoretical [min]', y = 'sample [min]')


# QQ-plot für Minuten

e_fit_mass_zone_2_s <- fitdistr(zone_2_TimeDiff$diff_secs, "exponential")
lnrm_fit_mass_zone_2_s <- fitdistr(zone_2_TimeDiff$diff_secs, "lognormal")
w_fit_mass_zone_2_s <- fitdistr(zone_2_TimeDiff$diff_secs, "weibull")

ggplot(zone_2_TimeDiff, aes(sample = diff_secs)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_zone_2_s$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qlnorm,
          dparams = list(lnrm_fit_mass_zone_2_s$estimate[1], lnrm_fit_mass_zone_2_s$estimate[2]),
          geom = "line",
          aes(color = "log normal")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_zone_2_s$estimate[1], w_fit_mass_zone_2_s$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "log normal" = "green",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "log normal",
                                "Weibull")) +
  labs(title = "Q-Q Plot in seconds",  x = 'theoretical [sec]', y = 'sample [sec]')



gDistTimeZ2 <- fitdistr(zone_2_TimeDiff$diff_mins, "log-normal")

gDistTimeZ2$estimate[1]
gDistTimeZ2$estimate[2]

```

#### 7.2.2 Result of zone 1

The best fit is the "exponential" distribution.
```{r}
# QQ-plot für Sekunden

zone_1_TimeDiff

e_fit_mass_zone_1_s <- fitdistr(zone_1_TimeDiff$diff_secs, "exponential")
w_fit_mass_zone_1_s <- fitdistr(zone_1_TimeDiff$diff_secs, "weibull")

ggplot(zone_1_TimeDiff, aes(sample = diff_secs)) +
  stat_qq(distribution = qexp,
          dparams = list(e_fit_mass_zone_1_s$estimate[1]),
          geom = "line",
          aes(color = "exponential")) +
  stat_qq(distribution = qweibull,
          dparams = list(w_fit_mass_zone_1_s$estimate[1], w_fit_mass_zone_1_s$estimate[2]),
          geom = "line",
          aes(color = "Weibull")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "Weibull" = "orange"),
                     breaks = c("exponential",
                                "Weibull")) +
  labs(title = "Q-Q Plot in seconds",x = 'theoretical [sec]', y = 'sample [sec]')

show(zone_1_TimeDiff)
  
# Ergebniss 1 : Die Verteilung die am besten passt ist die exponentialverteilung.

gDistTimeZ1 <- fitdistr(zone_2_TimeDiff$diff_mins, "exponential")
gDistTimeZ1$estimate[1]
```

## 8. Montecarlo

Because the previous test is ambiguous, we test if the sample could be normal distributed. We assume $(H_0)$ that the speed of `out_2` is normal distributed with significant level $95\%$. $H_1$ is that the speed of `out_2` of the data set is not normal distributed. Therefore, we compare 20 smples of random generated normal distributed quantiles vs. the theoretical quantiles. If there is a similar QQ-plot compared to the plot with the actual speed data, then we can assume that the speed of out_2 is normal distributed.

The comparison of the artificial created data sample and the speed data `out_2` let us assume, not to deny $H_0$. In addition, it is important to note that most of the speed values have not been estimated significantly differently with the normal distribution.
```{r echo = TRUE, fig.width = 6}

rows <- 20
cols <- length(out_2$speed)
random_dnorm_matrix <- matrix(nrow = rows, ncol = cols)
for (i in 1:rows){
  random_dnorm_matrix[i,] <- c(rnorm(cols, mean = nrm_fit_speed_out_2$estimate[1], sd = nrm_fit_speed_out_2$estimate[2]))
}

for (j in 1:rows){
  p <- ggplot2::qplot(sample = random_dnorm_matrix[j,], geom = "blank", xlab = "theoretical quantiles", ylab = "sample quantiles") +
    stat_qq(distribution = qnorm, dparams = list(nrm_fit_speed_out_2$estimate[1], nrm_fit_speed_out_2$estimate[2]), color = "steelblue", size = 3) +
    stat_qq_line(distribution = qnorm, dparams = list(nrm_fit_speed_out_2$estimate[1], nrm_fit_speed_out_2$estimate[2]), color = "red") +
    labs(title = paste0("Random Gaussian distributed Speed out_2 (img ", j, ")"), x = "theoretical quantiles [m/s]", y = "sample quantiles [m/s]")
 print(p)
}
```


## 9. Monte-Carlo Simulation

To get $P(S \geq 1)$ where $s = $ 'num of rocks through the net in a 1-year time frame', we set up a Monte-Carlo simulation to count, how many times a rock has fallen through the net considering numerous one-year simulations. In addition, we set up multiple simulations in order to compare them with each other. We assume, that the street will get closed after a rock fell through the net. Therefore, the simulation of one year will not continue to get probably another rockfall.

The simulation also takes into account the probability of a car accident, i.e. the probability that a car will be hit by a stone if it falls through the net. To this end, we at the Federal Statistical Office (BfS) have taken into account the driving density per hour on Swiss roads. We assume 3 different cases:

1. all 1600 cars are equally distributed on the day
2. we assume the highest traffic density over every hour of the day according to BfS
3. we consider the hour in which the stone falls down and the traffic density of the BfS

The result of the third variant should be taken with caution, as we take into account the time intervals between two events for the simulation, and not the time of day itself.

In order to obtain appropriate values for a rockfall, we obtain a random value corresponding to the distribution of the individual sources in terms of mass and speed, from which the energy can be calculated $energy[kJ] = 0.5*m*v^2$. If a stone falls into the net with more than 1000 kJ, then the rock falls onto the street. If rockfall has between 500 and 1000 kJ, then the stability of the net depends on the weight that is already in the net. It breaks through when the weight is more than 2000 kg. We assume that on every day, the net will be cleared on 08:00.
```{r echo = TRUE}
rock_event <- function(speed, mass){
  broken <- FALSE
  energy <- (0.5 * mass * speed^2) / 1000
  if (energy >= 1000){
    broken <- TRUE
  } else if (energy >= 500){
    if (mass_in_net >= 2000){
      broken <- TRUE
    }
  }
  if (broken){
    num_rock_through_net <<- num_rock_through_net + 1
    events_current_year <<- events_current_year + 1
  }
  mass_in_net <<- mass_in_net + mass
  return(broken)
}

net_clearing_process <- function(now, event_time_delta, clearing_time_minute, day_minutes){
  current_time <- now %% day_minutes
  event_time = current_time + event_time_delta
  if (current_time < clearing_time_minute){
    #day_time before clearing_time
    if (event_time >= clearing_time_minute){
      #clearing_time lies between current_time & event_time
      mass_in_net <<- 0
    }
  } else {
    #day_time after clearing_time
    if (event_time >= (day_minutes + clearing_time_minute))
      #clearing_time lies between current_time & event_time
      mass_in_net <<- 0
  }
}

car_hitting <- function(event_time, day_minutes){
  current_time <- event_time %% day_minutes
  #current_hour in integer, 8 O'clock = 08
  current_hour <- base::floor(current_time / 60)
  if (current_hour == 0) {
    current_hour = 24
  }
  #runif generates a random number between 0 and 1. If the number is below the expected value the car gets hit.
  hit_number <- runif(1, min = 0, max = 1)
  #Simulation with traffic data of the institute of statistics:
  if (hit_number <= total_in_danger$exp_car_per_hour[current_hour]) {
     car_hit_stat <<- car_hit_stat + 1
  }

  #Simulation with evenly distributed traffic:
  if (hit_number <= 0.008166) {
    car_hit_even <<- car_hit_even + 1
  }

  #Simulation with max traffic througout the day:
  if (hit_number <= 0.0175843437) {
    car_hit_max <<- car_hit_max + 1
  }

}
#create dataframe for probability counter
pro_counter <<- NULL


monte_carlo_rockfall <- function(num_of_years, clearing_time_hour, simulation_id){
  
  day_minutes <- 24 * 60
  year_minutes <- day_minutes * 365
  clearing_time_minute <- clearing_time_hour * 60
  
  
  
  for (year in 1:num_of_years){
    current_year <<- year
    events_current_year <<- 0
    mass_in_net <<- 0
    now <- 0
    time_to_next_event_out_1 <- rexp(1, rate = gDistTimeZ1$estimate[1])
    time_to_next_event_out_2 <- rlnorm(1, meanlog = gDistTimeZ2$estimate[1], sdlog = gDistTimeZ2$estimate[2])
    while (now < year_minutes){
      
      if (time_to_next_event_out_1 < time_to_next_event_out_2){
        net_clearing_process(now, time_to_next_event_out_1, clearing_time_minute, day_minutes)
        time_to_next_event_out_2 <- time_to_next_event_out_2 - time_to_next_event_out_1
        now <- now + time_to_next_event_out_1
        time_to_next_event_out_1 <- rexp(1, rate = gDistTimeZ1$estimate[1])
        speed_1 <- rnorm(1, mean = nrm_fit_speed_out_1$estimate[1], sd = nrm_fit_speed_out_1$estimate[2])
        mass_1 <- rlnorm(1, meanlog = lnrm_fit_mass_out_1$estimate[1], sdlog = lnrm_fit_mass_out_1$estimate[2])
        broken <- rock_event(speed_1, mass_1)
        if (broken){
          car_hitting(now, day_minutes)
          break
        }

      } else {
        net_clearing_process(now, time_to_next_event_out_2, clearing_time_minute, day_minutes)
        time_to_next_event_out_1 <- time_to_next_event_out_1 - time_to_next_event_out_2
        now <- now + time_to_next_event_out_2
        time_to_next_event_out_2 <- rlnorm(1, meanlog = gDistTimeZ2$estimate[1], sdlog = gDistTimeZ2$estimate[2])
        speed_2 <- rnorm(1, mean = nrm_fit_speed_out_2$estimate[1], sd = nrm_fit_speed_out_2$estimate[2])
        mass_2 <- rexp(1, rate = e_fit_mass_out_2$estimate[1])
        broken <- rock_event(speed_2, mass_2)
        if (broken){
          car_hitting(now, day_minutes)
          break
        }
      }
    }
    yearly_prob_rock_through_net_mc <- num_rock_through_net / year
    yearly_prob_car_hit_stat_mc <- car_hit_stat / year
    yearly_prob_car_hit_even_my <- car_hit_even / year
    yearly_prob_car_hit_max_mc <- car_hit_max / year
    
      
    pro_counter <<- rbind(pro_counter, data.frame(current_year, events_current_year, num_rock_through_net, yearly_prob_rock_through_net_mc, yearly_prob_car_hit_stat_mc, yearly_prob_car_hit_even_my, yearly_prob_car_hit_max_mc ))
  }
  prob_rock_through_net_mc <- num_rock_through_net / num_of_years
  prob_car_hit_stat_mc <- car_hit_stat / num_of_years
  prob_car_hit_even_mc <- car_hit_even / num_of_years
  prob_car_hit_max_mc <- car_hit_max / num_of_years

  file_pro_counter <- paste0('./RData/pro_counter_', num_of_years,'_years.rda')
  save(pro_counter, file = file_pro_counter)

  log_text <- paste('simulation_id:', simulation_id,
                    '\nrock through net: ', num_rock_through_net,
                    '\nprobability rock through net: ', prob_rock_through_net_mc,
                    '\ncar hit stat:', car_hit_stat, '(Calculated with traffic data of the swiss institute of statistics)',
                    '\nprobability car hit stat:', prob_car_hit_stat_mc,
                    '\ncar hit even:', car_hit_even, '(Calculated with an evenly distributed traffic)',
                    '\nprobability car hit even:', prob_car_hit_even_mc,
                    '\ncar hit max:', car_hit_max, '(Calculated with maximum traffic at all times)',
                    '\nprobability car hit max:', prob_car_hit_max_mc, '\n\n')

  cat(log_text, file = "./Log/simulation.log", append = TRUE)

  result_frame <- data.frame('simulation_id' = simulation_id,
                       'num_rock_through_net' = num_rock_through_net,
                       'prob_rock_through_net_mc' = prob_rock_through_net_mc,
                       'car_hit_stat' = car_hit_stat,
                       'prob_car_hit_stat_mc' = prob_car_hit_stat_mc,
                       'car_hit_even' = car_hit_even,
                       'prob_car_hit_even_mc' = prob_car_hit_even_mc,
                       'car_hit_max' = car_hit_max,
                       'prob_car_hit_max_mc' = prob_car_hit_max_mc,
                       'num_of_years' = num_of_years)
  return(result_frame)
  
}

simulation_controller <- function(num_of_simulations, num_of_years, clearing_time_hour){
  result <- data.frame('simulation_id' = integer(),
                       'num_rock_through_net' = integer(),
                       'prob_rock_through_net_mc' = double(),
                       'car_hit_stat' = integer(),
                       'prob_car_hit_stat_mc' = double(),
                       'car_hit_even' = integer(),
                       'prob_car_hit_even_mc' = double(),
                       'car_hit_max' = integer(),
                       'prob_car_hit_max_mc' = double(),
                       'num_of_years' = integer())
  file_path <- paste0('./RData/monte_carlo_rockfall_', num_of_simulations, '_simulations_', num_of_years, '_years.rda')
  save(result, file = file_path)
  for (simulation in 1:num_of_simulations){
    simulation_result <- monte_carlo_rockfall(num_of_years, clearing_time_hour, simulation)
    load(file = file_path)
    result <- rbind(result, simulation_result)
    save(result, file = file_path)

    #reset global variables for next simulation
    car_hit_stat <<- 0
    car_hit_even <<- 0
    car_hit_max <<- 0
    num_rock_through_net <<- 0
  }
}

car_hit_stat <- 0
car_hit_even <- 0
car_hit_max <- 0
num_rock_through_net <- 0
mass_in_net <- 0
load(file = './RData/DeltaT1Exponanitial.RData')
load(file = './RData/DeltaT2LogNormal.RData')


#simulation_controller(num_of_simulations = 1, num_of_years = 100, clearing_time_hour = 8)
```

## 10. Simulation Results

### 10.1 Result Table

`simulation_id` &rarr; 1 - 100 simulations of 100,000 one year runs

`num_rock_through_net` &rarr; number of rocks through net in one simulation

`prob_rock_through_net_mc` &rarr; $P(S \geq 1)$

`car_hit_stat` &rarr; number of car hits in one simulation respecting the day-time of breakthrough and day-time traffic density statistics of the BfS

`prob_car_hit_stat_mc` &rarr; $P(B = 1)$ where $b =$ 'car hit during one year' considering `car_hit_stat`

`car_hit_even` &rarr; number of car hits in one simulation assuming the traffic density is uniform distributed

`prob_car_hit_even_mc` &rarr; $P(E = 1)$ where $e =$ 'car hit during one year' considering `car_hit_even`

`car_hit_max` &rarr; number of car hits in one simulation assuming to have max traffic density respecting the statistics of the BfS

`prob_car_hit_max_mc` &rarr; $P(M = 1)$ where $m =$ 'car hit during one year' considering `car_hit_max`

`num_of_years` &rarr; number of simulated same years in one simulation

```{r echo = TRUE}
load(file = './RData/monte_carlo_rockfall_100_simulations_1e+05_years.rda')
result
```

### 10.2 Statistical Characteristics

Information about the data collected by running the simulation 100 times with 100,000 years of simulation each.
The values are rounded to two decimal places. Therefore, all `prob_*` are not presented appropriate. Details about the specific probabilities are shown in the next section.
```{r echo = TRUE}
#describeFast(result)
psych::describe(result[-1]) %>%
  dplyr::select(-vars, -n)
#data doesn't show probability correctly (rounded to 0.00)
```

### 10.3 Probabilities

Here we present the statistical characteristics of the probabilities we got from the simulations.
```{r echo = TRUE}
probs <- matrix(c(mean(result$prob_rock_through_net_mc), sd(result$prob_rock_through_net_mc), median(result$prob_rock_through_net_mc), min(result$prob_rock_through_net_mc), max(result$prob_rock_through_net_mc), mean(result$prob_car_hit_stat_mc), sd(result$prob_car_hit_stat_mc), median(result$prob_car_hit_stat_mc), min(result$prob_car_hit_stat_mc), max(result$prob_car_hit_stat_mc), mean(result$prob_car_hit_even_mc), sd(result$prob_car_hit_even_mc), median(result$prob_car_hit_even_mc), min(result$prob_car_hit_even_mc), max(result$prob_car_hit_even_mc), mean(result$prob_car_hit_max_mc), sd(result$prob_car_hit_max_mc), median(result$prob_car_hit_max_mc), min(result$prob_car_hit_max_mc), max(result$prob_car_hit_max_mc)), ncol = 5, byrow = TRUE)
colnames(probs) <- c('mean', 'sd', 'median', 'min', 'max')
rownames(probs) <- c('prob_rock_through_net_mc', 'prob_car_hit_stat_mc', 'prob_car_hit_even_mc', 'prob_car_hit_max_mc')
probs <- as.table(probs)
probs
```
The mean and median of the probability that a vehicle will be hit is above the predefined threshold 1.e-04 in all three cases. 

**For security reasons, we strongly recommend to close the road section!**

### 10.4 Simulation Probability Distribution

```{r echo = TRUE, fig.width = 10}
result_dist <- result %>%
  dplyr::select(prob_car_hit_stat_mc, prob_car_hit_even_mc, prob_car_hit_max_mc) %>%
  tidyr::gather(key = 'prob_type', value = 'prob')

ggplot(data = result_dist, mapping = aes(x = prob, group = prob_type, fill = prob_type)) +
  geom_density(adjust = 1.5, alpha = 0.4) +
  labs(title = 'Monte-Carlo Probability Distribution')
```
**Read description**: The graph shows the distributions of the probability of a vehicle being hit by a rockfall. The blue area shows the distribution of a hit if the traffic is statistically distributed. The red area shows the probability of a hit if the traffic is uniformly distributed. If the maximum traffic volume is constant the probability distribution is located much higher on the x-axis. (green line).


### 10.5 Car Hit Prob Simulated vs. Calculated

In a final step we calculate $P(E = 1 | S)$ to compare the calculated vs. the simulated value, where we assume that the traffic density is uniform distributed. We calculate it with the mean of `prob_rock_through_net_mc`.

```{r echo = TRUE}
prob_car_hit <- mean(result$prob_rock_through_net_mc) * e_car_hit
prob_car_hit
```
We can see, that the simulated (1.3380e-04) and calculated (1.4974e-04) probabilites approach each other. Both values are clearly above the pre-defined threshold of 1.e-04.

### 10.6 Convergence of Probability during Simulation
It is important to know if the monte-carlo simulation converges to a certain value. The below plot shows how the probability levelled of at about 0.017 or 1.7% after about 500 thousend simulated years.
```{r echo = TRUE}
#plot convergence of probability as time passes on
load(file = './RData/pro_counter_1e+05_years.rda')

ggplot(data = pro_counter)+
  geom_line(mapping = aes(x = pro_counter$current_year,  y = pro_counter$yearly_prob_rock_through_net_mc), color = 'orange')+
  labs(title = " Convergence of Probability of 'breakthrough' during Simulation", x = "Simulated years", y = "Probability")+
  geom_hline(yintercept=0.0162, linetype="dashed", color = "red") 
```
Read description: The graph (above) shows how the calculated probability of the Monte Carlo simulation levels off during the simulation. The value converges to 0.017, or 1.7%. The red line describes the calculated probability of a breakthrough (0.0162 or 1.62 %).

## 11. Conclusion

By calculating the probability of a rockfall with deadly concequences and also simulating 10 Million years to get a probability of a car getting hit by a rock as well, we can conclude that the probability of a death is to high. The probability is in all cases (calculated and simulated) above the given reference value of 1.e-04. For this reason we advise to close the main road in Schiers until the safety nets are completely replaced, to inform the population about the decision and to publish the findings of the notebook.


Written by:

- Lukas Gehrig, Data Science Student B.Sc. FHNW
- Riccard Nef, Data Science Student B.Sc. FHNW
- Roman Studer,  Data Science Student B.Sc. FHNW









